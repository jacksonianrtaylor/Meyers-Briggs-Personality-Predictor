# both: bag of words split...

# version with:
#  tokens_2 = [WordNetLemmatizer().lemmatize(token.lower()) for token in tokens_1 
#              if WordNetLemmatizer().lemmatize(token.lower()) in word_bank]
# Full compute time: 6.870408161481222 Minutes


# version with:
#  tokens_2 = [WordNetLemmatizer().lemmatize(token.lower()) for token in tokens_1 
#              if token.isalpha() and WordNetLemmatizer().lemmatize(token.lower()) not in stopwords.words('english')] 
#  Full compute time: 9.52508595387141 Minutes


# version with updates bag of words:
# Full compute time: 6.733085672060649 Minutes


# misc:
# why is a variables like bag of words not updated in the above function to save processing??? 

# the bag of words and the word_bank could be sorted
# the wordbank only needs to be sorted once but the bag of words is per user 

# why is the conditon in the following function search the word bank???
# Is it faster than confirming if its not in stopwords and is a alpha numerical???

# can introduce a new variable, bag_of_words_dict that adds all the copies of all the words/terms tokens_2
# it is a dictionary organized like data_by_type